gamma_tilde=filter_obj$gamma_tilde
# MSE filter applied to original (non-stationary) data: derived from target filter and MA inversion
gamma_mse=filter_obj$gamma_mse
# Convolution of integration and Wold decomposition, see section 5.3 Wildi (2025)
Xi_tilde=filter_obj$Xi_tilde
# Integration (sum), see section 5.3 Wildi (2025)
Sigma=filter_obj$Sigma
# Inverse of integration, see section 5.3 Wildi (2025)
Delta=filter_obj$Delta
# Wold decomposition (MA-inversion of AR(1) with parameter a1=0.3): depends on a1
Xi=filter_obj$Xi
# Two-sided target: used for computing sample MSEs below
hp_two=filter_obj$hp_two
# Classic concurrent HP: we want to replicate the latter's holding-time by SSA
hp_trend=filter_obj$hp_trend
# Specify rho1 in HT constraint, see Wildi (2025) section 5.4
# 1. Compute rho_mse the lag-one ACF of the MSE nowcast
#   The holding time constraint is expressed in (stationary) first differences of the I(1) process
#   The (differenced) data is an AR(1) and therefore we have to compute the convolution of filters and MA-inversion, see tutorial 2
rho_mse<-compute_holding_time_func(Xi%*%gamma_mse)$rho_ff1
# We also use the MA-inversion Xi%*%hp_trend for deriving the lag-one ACF of HP-C
rho_hp_concurrent<-as.double(compute_holding_time_func(Xi%*%hp_trend)$rho_ff1)
# The lag-one ACF of the MSE nowcast is small: the filter generates many noisy zero-crossings (see last plot below)
rho_mse
# We set rho1=rho_hp_concurrent in the HT constraint
# Research question/assumption: impose same smoothness as HP-C; should outperform HP-C in terms of MSE; without loosing too much vs. gamma_mse
# Increasing smoothness leads to worse MSE performances (Accuracy-Smoothness dilemma)
rho1<-rho_hp_concurrent
if (F)
{
# We can impose a holding time 50% larger than HP-C to equal MSE performances:
#   SSA will replicate MSE performances of HP-C with less noisy zero-crossings
ht<-1.5*as.double(compute_holding_time_func(Xi%*%hp_trend)$ht)
rho1<-compute_rho_from_ht(ht)$rho
}
#----------------------------
# Compute SSA solution, seee Wildi (2025) sections 5.3 and 5.4
#   Use optim to determine optimal Lagrangian multiplier lambda numerically
#     The optimal Lagrangian ensures compliance with the HT constraint
#   Initialize lambda with 0: MSE benchmark (numerical optimization must improve upon MSE-initialization)
lambda<-0
opt_obj<-optim(lambda,b_optim,lambda,gamma_mse,Xi,Sigma,Xi_tilde,M,B,gamma_tilde,rho1)
# Optimized lambda
lambda_opt<-opt_obj$par
# Compute cointegrated I(1) SSA solution for the optimal Lagrangian parameter lambda_opt
bk_obj<-bk_int_func(lambda_opt,gamma_mse,Xi,Sigma,Xi_tilde,M,B,gamma_tilde)
# Lag-one ACF: matches HT constraint rho1 as desired
bk_obj$rho_yy
rho1
# Correlation with target (based on synthetic stationary series, see Wildi (2025), section 5.3)
bk_obj$rho_yz
# MSE: with respect to causal MSE predictor
bk_obj$mse_yz
# SSA filter: b applied to x, i.e., INDPRO
b_x<-bk_obj$b_x
# b applied to eps in synthetic series
b_eps<-bk_obj$b_eps
# If Xi=Identity then b_x=b_eps
par(mfrow=c(1,2))
ts.plot(b_eps,main="B as applied to epsilon")
ts.plot(b_x,main="b as applied to INDPRO")
# Check cointegration constraint: difference should vanish, see Wildi (2025) section 5.3
sum(b_x)-sum(gamma_mse)
# Check HT constraint: difference should vanish for optimal Lagrangian lambda_opt (up to rounding error)
bk_obj$rho_yy-rho1
# Plot the filters
par(mfrow=c(1,2))
colo<-c("violet","green","blue","red")
mplot<-cbind(hp_two,c(gamma_mse,rep(0,L-1)),c(b_x,rep(0,L-1)),c(hp_trend,rep(0,L-1)))
colnames(mplot)<-c("HP-two","MSE","SSA","HP-C")
plot(mplot[,1],main="Trend filters",axes=F,type="l",ylab="",xlab="Lags",col=colo[1],lwd=1,ylim=c(min(mplot),max(mplot)))
abline(h=0)
for (i in 1:ncol(mplot))
{
lines(mplot[,i],col=colo[i])
mtext(colnames(mplot)[i],line=-i,col=colo[i])
}
axis(1,at=1:nrow(mplot),labels=0:(nrow(mplot)-1))
axis(2)
box()
mplot<-cbind(hp_two,c(gamma_mse,rep(0,L-1)),c(b_x,rep(0,L-1)),c(hp_trend,rep(0,L-1)))[1:30,]
colnames(mplot)<-c("HP-two","MSE","SSA","HP-C")
plot(mplot[,1],main="",axes=F,type="l",ylab="",xlab="",col=colo[1],lwd=1,ylim=c(min(mplot[,"HP-C"]),max(mplot[,"SSA"])))
abline(h=0)
for (i in 1:ncol(mplot))
{
lines(mplot[,i],col=colo[i])
mtext(colnames(mplot)[i],line=-i,col=colo[i])
}
axis(1,at=1:nrow(mplot),labels=0:(nrow(mplot)-1))
axis(2)
box()
#----------------------------------------
# Filter data
y_ssa<-filter(x_tilde,b_x,side=1)
y_hp_concurrent<-filter(x_tilde,hp_trend,side=1)
y_mse<-filter(x_tilde,gamma_mse,side=1)
y_target<-filter(x_tilde,hp_two,side=2)
colo<-c("black","violet","green","blue","red")
# Plots
# First plot: data in levels
#   -Classic HP (HP-C) over/under-shoots at peaks and dips; it also systematically lags (right shifted);
#   -MSE is closest to target, by design; SSA is as smooth as HP-C but closer to target
par(mfrow=c(1,1))
anf<-L+100
enf<-length(x_tilde)
mplot<-cbind(x_tilde,y_target,y_mse,y_ssa,y_hp_concurrent)[anf:enf,]
colnames(mplot)<-c("Data","Target: HP-two","MSE: HP-one","SSA","HP-C")
plot(mplot[,1],main="Data and trends",axes=F,type="l",xlab="",ylab="",col=colo[1],lwd=1)
for (i in 1:ncol(mplot))
{
lines(mplot[,i],col=colo[i])
mtext(colnames(mplot)[i],line=-i,col=colo[i])
}
axis(1,at=1:nrow(mplot),labels=index(y_xts)[(anf):length(y_xts)])
axis(2)
box()
# Second plot: data in differences
#   -MSE (top blot) is very noisy and generates lots of zero-crossings
#   -HP-C and SSA are much smoother and both track last three recessions quite well with few false alarms during longer expansions
anf<-L+100
enf<-length(x_tilde)-1
mplot<-apply(cbind(x_tilde,y_target,y_mse,y_ssa,y_hp_concurrent),2,diff)[anf:enf,]
colnames(mplot)<-c("Diff-Data","Target: HP-two","MSE: HP-one","SSA","HP-C")
par(mfrow=c(3,1))
# Select target and MSE
select_vec<-c(2,3)
plot(mplot[,select_vec[1]],main="Zero Crossings MSE",axes=F,type="l",xlab="",ylab="",col=colo[select_vec[1]],lwd=1,ylim=c(-0.013,0.006))
abline(v=1+which(sign(mplot[2:nrow(mplot),select_vec[2]])!=sign(mplot[1:(nrow(mplot)-1),select_vec[2]])),col=colo[select_vec[2]],lwd=1,lty=2)
abline(v=1+which(sign(mplot[2:nrow(mplot),select_vec[2]])!=sign(mplot[1:(nrow(mplot)-1),select_vec[2]])),col="black",lwd=1,lty=2)
abline(h=0)
for (i in 1:length(select_vec))
{
lines(mplot[,select_vec[i]],col=colo[select_vec[i]])
mtext(colnames(mplot)[select_vec[i]],line=-i,col=colo[select_vec[i]])
}
axis(1,at=1:nrow(mplot),labels=index(y_xts)[(anf+1):length(y_xts)])
axis(2)
box()
# Select target and HP-C
select_vec<-c(2,5)
plot(mplot[,select_vec[1]],main="Zero Crossings HP-C",axes=F,type="l",xlab="",ylab="",col=colo[select_vec[1]],lwd=1,ylim=c(-0.013,0.006))
abline(v=1+which(sign(mplot[2:nrow(mplot),select_vec[2]])!=sign(mplot[1:(nrow(mplot)-1),select_vec[2]])),col=colo[select_vec[2]],lwd=1,lty=2)
abline(v=1+which(sign(mplot[2:nrow(mplot),select_vec[2]])!=sign(mplot[1:(nrow(mplot)-1),select_vec[2]])),col="black",lwd=1,lty=2)
abline(h=0)
for (i in 1:length(select_vec))
{
lines(mplot[,select_vec[i]],col=colo[select_vec[i]])
mtext(colnames(mplot)[select_vec[i]],line=-i,col=colo[select_vec[i]])
}
axis(1,at=1:nrow(mplot),labels=index(y_xts)[(anf+1):length(y_xts)])
axis(2)
box()
# Select target and SSA
select_vec<-c(2,4)
plot(mplot[,select_vec[1]],main="Zero Crossings SSA",axes=F,type="l",xlab="",ylab="",col=colo[select_vec[1]],lwd=1,ylim=c(-0.013,0.006))
abline(v=1+which(sign(mplot[2:nrow(mplot),select_vec[2]])!=sign(mplot[1:(nrow(mplot)-1),select_vec[2]])),col=colo[select_vec[2]],lwd=1,lty=2)
abline(v=1+which(sign(mplot[2:nrow(mplot),select_vec[2]])!=sign(mplot[1:(nrow(mplot)-1),select_vec[2]])),col="black",lwd=1,lty=2)
abline(h=0)
for (i in 1:length(select_vec))
{
lines(mplot[,select_vec[i]],col=colo[select_vec[i]])
mtext(colnames(mplot)[select_vec[i]],line=-i,col=colo[select_vec[i]])
}
axis(1,at=1:nrow(mplot),labels=index(y_xts)[(anf+1):length(y_xts)])
axis(2)
box()
#----------------------------------------
# Sample performances: we expect SSA to equal HP-C in terms of smoothness and to outperform in terms of MSE performances;
# MSE with respect to two-sided target
mean((y_target-y_mse)^2,na.rm=T)
mean((y_target-y_ssa)^2,na.rm=T)
mean((y_target-y_hp_concurrent)^2,na.rm=T)
# MSE of SSA with respect to one-sided MSE predictor (this should match bk_obj$mse_yz)
mean((y_mse-y_ssa)^2,na.rm=T)
bk_obj$mse_yz
compute_empirical_ht_func(diff(y_mse)[anf:enf])$empirical_ht
compute_holding_time_from_rho_func(rho_mse)$ht
compute_empirical_ht_func(diff(y_ssa)[anf:enf])$empirical_ht
compute_holding_time_from_rho_func(bk_obj$rho_yy)$ht
compute_empirical_ht_func(diff(y_hp_concurrent)[anf:enf])$empirical_ht
compute_holding_time_from_rho_func(rho_hp_concurrent)$ht
compute_empirical_ht_func(diff(y_target))$empirical_ht
mat_perf<-matrix(nrow=2,ncol=3)
mat_perf[1,]<-c(mean((y_target-y_mse)^2,na.rm=T),mean((y_target-y_ssa)^2,na.rm=T),mean((y_target-y_hp_concurrent)^2,na.rm=T))
mat_perf[2,]<-c(compute_empirical_ht_func(diff(y_mse)[anf:enf])$empirical_ht,compute_empirical_ht_func(diff(y_ssa)[anf:enf])$empirical_ht,compute_empirical_ht_func(diff(y_hp_concurrent)[anf:enf])$empirical_ht)
colnames(mat_perf)<-c("MSE nowcast","SSA","HP-C")
rownames(mat_perf)<-c("Sample mean square error","Sample holding time")
# All performances at a glance:
mat_perf
compute_empirical_ht_func(diff(y_ssa)[anf:enf])$empirical_ht
Xi_tilde
Xi=filter_obj$Xi
Xi
gamma_mse
View(compute_system_filters_func)
a_vec<-a1
,b_vec<-b1
HP_obj<-HP_target_mse_modified_gap(2*(L-1)+1,lambda_hp)
hp_two<-HP_obj$target
HP_obj<-HP_target_mse_modified_gap(L,lambda_hp)
# Concurrent MSE estimate of bi-infinite HP assuming white noise (truncate symmetric filter)
#  HP_obj$hp_trend
gamma<-HP_obj$hp_mse
ts.plot(gamma)
hp_trend<-HP_obj$hp_trend
# Wold decompoistion (MA inversion)
xi<-ARMAtoMA(ar=a_vec,ma=b_vec,lag.max=L)
b_vec<-b1
HP_obj<-HP_target_mse_modified_gap(2*(L-1)+1,lambda_hp)
hp_two<-HP_obj$target
HP_obj<-HP_target_mse_modified_gap(L,lambda_hp)
# Concurrent MSE estimate of bi-infinite HP assuming white noise (truncate symmetric filter)
#  HP_obj$hp_trend
gamma<-HP_obj$hp_mse
ts.plot(gamma)
hp_trend<-HP_obj$hp_trend
# Wold decompoistion (MA inversion)
xi<-ARMAtoMA(ar=a_vec,ma=b_vec,lag.max=L)
# Compute all system matrices: all matrices are specified in Wildi (2025)
Xi<-NULL
for (i in 1:L)
Xi<-rbind(Xi,c(xi[i:1],rep(0,L-i)))#c(1,0,0),c(1,1,0),c(1,1,1)))
Xi
Sigma<-NULL
for (i in 1:L)
Sigma<-rbind(Sigma,c(rep(1,i),rep(0,L-i)))#c(1,0,0),c(1,1,0),c(1,1,1)))
Sigma
# Invert: gives Delta
Delta<-solve(Sigma)
Delta
Xi_tilde<-(Sigma)%*%Xi
Sigma
Xi
(rep(1,length(hp_two))
rep(1,length(hp_two))
xi_int<-conv_two_filt_func(rep(1,length(hp_two)),xi)$conv
# 2. Convolution target filter and MA-inversion
hp_xi<-conv_two_filt_func(hp_two,xi_int)$conv
hp_xi
L
hp_xi_causal<-hp_xi[L:(length(hp_xi))]
# 4. Deconvolute filt2 from filt1: filt1 is the convolution
gamma_mse<-deconvolute_func(hp_xi_causal,xi_int[1:L])$dec_filt
Xi%*%gamma_mse
rm(list=ls())
# inst_pack<-rownames(installed.packages())
# if (!"mFilter"%in%inst_pack)
#   install.packages("mFilter")
# if (!"xts"%in%inst_pack)
#   install.packages("xts")
# if (!"MTS"%in%inst_pack)
#   install.packages("MTS")
# if (!"sandwich"%in%inst_pack)
#   install.packages("sandwich")
# if (!"multDM"%in%inst_pack)
#   install.packages("multDM")
# if (!"fGarch"%in%inst_pack)
#   install.packages("fGarch")
# if (!"xtable"%in%inst_pack)
#   install.packages("xtable")
# Load the required R-libraries
# Standard filter package
library(mFilter)
# Multivariate time series: VARMA model for macro indicators: used here for simulation purposes only
library(MTS)
# HAC estimate of standard deviations in the presence of autocorrelation and heteroscedasticity
library(sandwich)
# Extended time series
library(xts)
# Library for Diebold-Mariano test of equal forecast performance
library(multDM)
# GARCH model: for improving regression estimates
library(fGarch)
# Library for tables
library(xtable)
# # Load the relevant M-SSA functionalities
# # M-SSA functions
# source(paste(getwd(),"/R/functions_MSSA.r",sep=""))
# # Load signal extraction functions used for JBCY paper (relies on mFilter)
# source(paste(getwd(),"/R/HP_JBCY_functions.r",sep=""))
# # Utility functions for M-SSA, see tutorial
# source(paste(getwd(),"/R/M_SSA_utility_functions.r",sep=""))
# # Set of performance metrics and tests of unequal predictability
# #source(paste(getwd(),"/R/performance_statistics_functions.r",sep=""))
# main working directory should be ~/GitHub/BIP-predictor
path.main<-getwd()
path.pgm<-paste(path.main,"/R/",sep="")
path.out<-paste(path.main,"/Latex/",sep="")
path.sweave<-paste(path.main,"/Sweave/",sep="")
path.data<-paste(path.main,"/Data/",sep="")
# Savd results from an empirical analysis of S&P500
path.result<-paste(path.main,"/Results/",sep="")
#fig_size<-4
#------------------------------
recompute_results<-F
# Load data
data_file_name<-c("Data_HWI_2025_02.csv","gdp_2025_02.csv")
# Original (un-transformed) indicators
data_quarterly<-read.csv(paste(getwd(),"/Data/",data_file_name[2],sep=""))
BIP_original<-data_quarterly[,"BIP"]
# Transformed indicators: differences, trimmed, standardized
load(file=paste(getwd(),"\\Data\\macro",sep=""))
select_vec_multi<-c("BIP","ip","ifo_c","ESI","spr_10y_3m")
x_mat<-data[,select_vec_multi]
rownames(x_mat)<-rownames(data)
n<-dim(x_mat)[2]
# Number of observations
len<-dim(x_mat)[1]
ht_mssa_vec<-c(6.380160,  6.738270,   7.232453,   7.225927,   7.033768)
f_excess<-c(5,rep(4,length(select_vec_multi)-1))
# In-sample span for VAR
date_to_fit<-"2008"
# Model orders
p<-1
q<-0
# Filter length
L<-31
# Publication lag: one quarter for BIP
lag_vec<-c(1,rep(0,ncol(x_mat)-1))
# Plot BIP and diff-log BIP
# file = "./Figures/data.pdf"
# pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(1,1))
# Plot the data
# The real-time BIP (red) is lagging the target (black) by lag_vec[1] quarters (publication lag)
mplot<-x_mat
colo<-c("black",rainbow(ncol(data)-1))
main_title<-"Data: standardized diff-log, Pandemic outliers trimmed"
plot(mplot[,1],main=main_title,axes=F,type="l",xlab="",ylab="",col=colo[1],lwd=1,ylim=c(min(na.exclude(mplot)),max(na.exclude(mplot))))
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 1:ncol(mplot))
{
lines(mplot[,i],col=colo[i],lwd=1,lty=1)
mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=c(1,4*1:(nrow(mplot)/4)),labels=rownames(mplot)[c(1,4*1:(nrow(mplot)/4))])
axis(2)
box()
# Plot BIP and diff-log BIP
# file = "./Figures/data_lags.pdf"
# pdf(file = paste(path.out,file,sep=""), paper = "special", width = 6, height = 6)
par(mfrow=c(1,2))
# Plot the data
# The real-time BIP (red) is lagging the target (black) by lag_vec[1] quarters (publication lag)
mplot<-x_mat[which(rownames(x_mat)>=2007&rownames(x_mat)<=2011),]
colo<-c("black",rainbow(ncol(data)-1))
main_title<-"Financial Crisis"
plot(mplot[,1],main=main_title,axes=F,type="l",xlab="",ylab="",col=colo[1],lwd=1,ylim=c(min(na.exclude(mplot)),max(na.exclude(mplot))))
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 1:ncol(mplot))
{
lines(mplot[,i],col=colo[i],lwd=1,lty=1)
mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=c(1,4*1:(nrow(mplot)/4)),labels=rownames(mplot)[c(1,4*1:(nrow(mplot)/4))])
axis(2)
box()
mplot<-x_mat[which(rownames(x_mat)>=2019&rownames(x_mat)<=2022),]
colo<-c("black",rainbow(ncol(data)-1))
main_title<-"Pandemic"
plot(mplot[,1],main=main_title,axes=F,type="l",xlab="",ylab="",col=colo[1],lwd=1,ylim=c(min(na.exclude(mplot)),max(na.exclude(mplot))))
mtext(colnames(mplot)[1],col=colo[1],line=-1)
for (i in 1:ncol(mplot))
{
lines(mplot[,i],col=colo[i],lwd=1,lty=1)
mtext(colnames(mplot)[i],col=colo[i],line=-i)
}
abline(h=0)
axis(1,at=c(1,4*1:(nrow(mplot)/4)),labels=rownames(mplot)[c(1,4*1:(nrow(mplot)/4))])
axis(2)
box()
x_mat_wc<-x_mat[which(rownames(x_mat)<2020|rownames(x_mat)>2021),]
#--------------------------------------------------------
# Section 2: dependence
#--------------------------------------------------------
# Set the data set, estimation & refinement parameters
# data_fit <- x_mat     # Full Sample
data_fit <- x_mat_wc  # Pre-Pandemic
# Set threshold higher to select simpler models
threshold <- 2.5
# Calculate IRFs for how many lags?
nlags <- 24
#--------------------------------------------------------
# Section 2.1: Examine refined VARMAs
#--------------------------------------------------------
# Search over VARMA[p,q] and compare AIC, BIC
p.max <- 6
q.max <- 1
VARMA.results <- as.data.frame(matrix(NA, nrow = p.max*(q.max+1), ncol = 6))
colnames(VARMA.results) <- c('p','q','Thresh','Nparam','aic','bic')
for (j in 1:p.max) {
for (k in 0:q.max) {
VARMA_obj<-VARMA(data_fit,p=j,q=k, prelim = T, details = F, thres = threshold)
VARMA.results$p[(j-1)*(q.max+1) + (k + 1)] <- VARMA_obj$ARorder
VARMA.results$q[(j-1)*(q.max+1) + (k + 1)] <- VARMA_obj$MAorder
VARMA.results$Thresh[(j-1)*(q.max+1) + (k + 1)] <- threshold
VARMA.results$Nparam[(j-1)*(q.max+1) + (k + 1)] <- sum(VARMA_obj$coef != 0)
VARMA.results$aic[(j-1)*(q.max+1) + (k + 1)] <- VARMA_obj$aic
VARMA.results$bic[(j-1)*(q.max+1) + (k + 1)] <- VARMA_obj$bic
}
}
VARMA.results
# Let's recover the IRFs
VARMA_30<-VARMA(data_fit,p=3,q=0, prelim = T, details = F, thres = threshold)
VARMAirf_3 <- VARMAirf(Phi = VARMA_30$Phi, Theta = VARMA_30$Theta,
Sigma = VARMA_30$Sigma, lag = nlags, orth = F)
VARMA_40<-VARMA(data_fit,p=4,q=0, prelim = T, details = F, thres = threshold)
VARMAirf_4 <- VARMAirf(Phi = VARMA_40$Phi, Theta = VARMA_40$Theta,
Sigma = VARMA_40$Sigma, lag = nlags, orth = F)
# The following recovers the IRF for BIP (because BIP is row 1 in $psi)
IRF_BIP_VARMA3 <- as.data.frame(t(matrix(VARMAirf_3$psi[1,], ncol = 1+nlags)))
names(IRF_BIP_VARMA3) <- labels(x_mat)[[2]]   # labels source of shocks
rownames(IRF_BIP_VARMA3) <- seq.int(from = 0, to = nlags) # labels lags
IRF_BIP_VARMA4 <- as.data.frame(t(matrix(VARMAirf_4$psi[1,], ncol = 1+nlags)))
names(IRF_BIP_VARMA4) <- labels(x_mat)[[2]]   # labels source of shocks
rownames(IRF_BIP_VARMA4) <- seq.int(from = 0, to = nlags) # labels lags
#--------------------------------------------------------
VARorder(data_fit, maxp = 6)
VARorder(data_fit, maxp = 8)
VARorder(data_fit, maxp = 12)
# In all three cases, LRstat and AIC want 4 lags but BIC and HQ prefer 1.
# The following recovers the IRF for BIP (because BIP is row 1 in $psi)
IRF_BIP_VAR1 <- as.data.frame(t(matrix(VAR1_MA$psi[1,], ncol = 1+nlags)))
IRF_BIP_VAR1 <- as.data.frame(t(matrix(VAR1_MA$psi[1,], ncol = 1+nlags)))
VARorder(data_fit, maxp = 6)
VARorder(data_fit, maxp = 8)
VARorder(data_fit, maxp = 12)
# In all three cases, LRstat and AIC want 4 lags but BIC and HQ prefer 1.
# Estimate the VAR parameters without restrictions
VAR1_obj <- VAR(x = data_fit, p = 1)
VAR4_obj <- VAR(x = data_fit, p = 4)
# Let's check out the MA coefficients implied by the two sets of estimates
# The IRFs are stored in 5x5 blocks of coefficients for lags 0 through 20
nlags <- 24
VAR1_MA <- VARpsi(Phi = VAR1_obj$Phi, lag = nlags)
VAR4_MA <- VARpsi(Phi = VAR4_obj$Phi, lag = nlags)
# The following recovers the IRF for BIP (because BIP is row 1 in $psi)
IRF_BIP_VAR1 <- as.data.frame(t(matrix(VAR1_MA$psi[1,], ncol = 1+nlags)))
names(IRF_BIP_VAR1) <- labels(x_mat)[[2]]   # labels source of shocks
rownames(IRF_BIP_VAR1) <- seq.int(from = 0, to = nlags) # labels lags
IRF_BIP_VAR4 <- as.data.frame(t(matrix(VAR4_MA$psi[1,], ncol = 1+nlags)))
names(IRF_BIP_VAR4) <- labels(x_mat)[[2]]   # labels source of shocks
rownames(IRF_BIP_VAR4) <- seq.int(from = 0, to = nlags) # labels lags
# parameters for BVAR
k <- 5   # dimension of system
V0_BVAR <- diag(rep(1,k))  # prior for Sigma_a matrix - K x K
lambda_BVAR <- 0.001  # scaling factor for precision matrix
# precision matrix of size (kp+1) x (kp+1)  if there's a constant
#   low values <=> low precision
# Try a BVAR(3)
p_BVAR <- 3
C_BVAR <- lambda_BVAR*diag(rep(1,k*p_BVAR+1))
BVAR3_obj <- BVAR(data_fit,
p = p_BVAR,
C = C_BVAR,
V0 = V0_BVAR)
C_BVAR
BVAR3_MA <- VARpsi(Phi = BVAR3_obj$Phi, lag = nlags)
IRF_BIP_BVAR3 <- as.data.frame(t(matrix(BVAR3_MA$psi[1,], ncol = 1+nlags)))
names(IRF_BIP_BVAR3) <- labels(x_mat)[[2]]   # labels source of shocks
rownames(IRF_BIP_BVAR3) <- seq.int(from = 0, to = nlags) # labels lags
# Try a BVAR(4)
lambda_BVAR <- 10  # scaling factor for precision matrix
p_BVAR <- 4
C_BVAR <- lambda_BVAR*diag(rep(1,k*p_BVAR+1))
BVAR4_obj <- BVAR(data_fit,
p = p_BVAR,
C = C_BVAR,
V0 = V0_BVAR)
BVAR4_MA <- VARpsi(Phi = BVAR4_obj$Phi, lag = nlags)
IRF_BIP_BVAR4 <- as.data.frame(t(matrix(BVAR4_MA$psi[1,], ncol = 1+nlags)))
names(IRF_BIP_BVAR4) <- labels(x_mat)[[2]]   # labels source of shocks
rownames(IRF_BIP_BVAR4) <- seq.int(from = 0, to = nlags) # labels lags
MyColours <- c("blue", "darkgreen", "red3", "gray80", "lightgreen", "orange")
# plot the results
for (j in names(IRF_BIP_VAR1)) {
plot(IRF_BIP_VAR1[[j]], type = 'l', col = MyColours[1], lwd = 3,
ylim = c(-0.25, 1),
ylab = 'IRF of GDP', xlab = "Lags", main = paste("Shocks to", j))
lines(IRF_BIP_VAR4[[j]], col = MyColours[2], lwd = 3)
lines(IRF_BIP_VARMA3[[j]], col = MyColours[3], lwd = 3)
lines(IRF_BIP_VARMA4[[j]], col = MyColours[4], lwd = 2)
lines(IRF_BIP_BVAR3[[j]], col = MyColours[5], lwd = 3)
lines(IRF_BIP_BVAR4[[j]], col = MyColours[6], lwd = 3)
abline(h = 0, lty = 3)
legend("topright",
legend = c("VAR(1)", "VAR(4)", "Reg. VAR(3)", "Reg. VAR(4)",
"BVAR(3)", "BVAR(4)"),
col = MyColours,
lwd = 3)
}
MyColours <- c("blue", "darkgreen", "red3", "gray80", "lightgreen", "orange")
# plot the results
for (j in names(IRF_BIP_VAR1)) {
plot(IRF_BIP_VAR1[[j]], type = 'l', col = MyColours[1], lwd = 3,
ylim = c(-0.25, 1),
ylab = 'IRF of GDP', xlab = "Lags", main = paste("Shocks to", j))
lines(IRF_BIP_VAR4[[j]], col = MyColours[2], lwd = 3)
lines(IRF_BIP_VARMA3[[j]], col = MyColours[3], lwd = 3)
lines(IRF_BIP_VARMA4[[j]], col = MyColours[4], lwd = 2)
lines(IRF_BIP_BVAR3[[j]], col = MyColours[5], lwd = 3)
lines(IRF_BIP_BVAR4[[j]], col = MyColours[6], lwd = 3)
abline(h = 0, lty = 3)
legend("topright",
legend = c("VAR(1)", "VAR(4)", "Reg. VAR(3)", "Reg. VAR(4)",
"BVAR(3)", "BVAR(4)"),
col = MyColours,
lwd = 3)
}
