for (j in 2:n)
{
mplot<-cbind(mplot,bk_mat[(j-1)*L+1:L,i])
}
ts.plot(mplot,main=paste("SSA applied to epsilon ",colnames(x_mat)[i],sep=""),col=rainbow(n))
}
# We can compare to bk_x_mat (the filter effectively applied to the data)
bk_x_mat<-MSSA_obj$bk_x_mat
par(mfrow=c(1,n))
for (i in 1:n)# i<-1
{
mplot<-bk_x_mat[1:L,i]
for (j in 2:n)
{
mplot<-cbind(mplot,bk_x_mat[(j-1)*L+1:L,i])
}
ts.plot(mplot,main=paste("MSSA applied to x ",colnames(x_mat)[i],sep=""),col=rainbow(n))
}
# The weights of bk_mat (applied to residuals) decay more slowly in this example, because the white noise
#   (WN) sequence of the MA-inversion is noisier than the original VAR(1) (the original data)
# We can now check the deconvolution claim: deconvolution of the MA-inversion xi from bk_mat gives bk_x_mat
deconv_M<-t(M_deconvolute_func(t(bk_mat),xi)$deconv)
# Check: both filters match perfectly (absolute error is zero)
max(abs(deconv_M-bk_x_mat))
# To summarize
# -bk_x_mat is the relevant filter in applications (the filter which is applied to the data)
# -bk_mat (as applied to the MA-inversion) is the proper solution to the M-SSA criterion:
#   -bk_x_mat is derived from bk_mat by deconvolution of the MA-inversion
# -bk_mat can provide some useful visual clues (impulse response) and helps when
#   trying to understand/interpret the M-SSA solution (explainability)
# M-SSA also computes the classic MSE filter called M-MSE:
#   -This is the design obtained by classic signal extraction (Wiener-Kolmogorov)
# First we look at the MSE-filter as applied to the VAR residuals (after MA-inversion of the VAR)
gammak_mse<-MSSA_obj$gammak_mse
par(mfrow=c(1,n))
for (i in 1:n)# i<-1
{
mplot<-gammak_mse[1:L,i]
for (j in 2:n)
{
mplot<-cbind(mplot,gammak_mse[(j-1)*L+1:L,i])
}
ts.plot(mplot,main=paste("M-MSE applied to eps ",colnames(x_mat)[i],sep=""),col=rainbow(n))
}
# Second we look at the MSE filter as applied to the original data (x_mat)
#   -If the HT constraint of M-SSA matches the HT of this filter, then M-SSA replicates M-MSE
#   -M-SSA is a generalization of classic signal extraction algorithms, allowing for a control of the HT (smoothness or noise suppression)
gammak_x_mse<-MSSA_obj$gammak_x_mse
par(mfrow=c(1,n))
for (i in 1:n)# i<-1
{
mplot<-gammak_x_mse[1:L,i]
for (j in 2:n)
{
mplot<-cbind(mplot,gammak_x_mse[(j-1)*L+1:L,i])
}
ts.plot(mplot,main=paste("M-MSE applied to x ",colnames(x_mat)[i],sep=""),col=rainbow(n))
}
# If the imposed HT of M-SSA is larger than the HT  of M-MSE, then the M-SSA filter will typically
#   decay more slowly towards zero than the M-MSE benchmark (stronger smoothing)
# We now look at the acausal target
par(mfrow=c(1,1))
ts.plot(t(gamma_target),col=rainbow(n),main=c("Target as applied to original data", "Right tail is mirrored to the left to obtain two-sided filter","For each series, the target assigns weight to that series only"))
abline(v=(1:n*(nrow(t(gamma_target))/n)))
# But we can also look at the acausal target as applied to the MA-inversion of the VAR
# The following plot displays the target (two-sided HP filter) as applied to VAR-residuals
#   (convolution of above target and MA-inversion xi)
gamma_target_long<-MSSA_obj$gammak_target
par(mfrow=c(1,1))
ts.plot(gamma_target_long,col=rainbow(n),main=c("Targets as assigned to MA-inversion of original data","The originally symmetric two-sided target appears less symmetric after convolution with MA-inversion","Weight can be assigned to multiple series if VAR is not diagonal"))
abline(v=(1:n*(nrow(gamma_target_long)/n)))
# Notes
# 1. We show both sides of the filter (after `mirroring')
# 2. For a particular series (say the first one, i=1), the convolution of the original target and
#     the MA-inversion xi may assign weight to (VAR-) residuals of other series in the presence of
#     cross-correlation (non-diagonal VAR)
# M-SSA also computes the (true) variance of the two-sided HP output (assuming no model misspecification)
var_target<-MSSA_obj$var_target
# We can compare variances on the diagonal of the matrix var_target with variances of M-MSE (below):
#   -The latter are smaller (because the causal MSE predictor is missing future epsilons)
# For this purpose, we need the system matrices M_tilde and I_tilde, see M-SSA paper for background
#   -The system matrices are defined as Kronecker-products of Sigma (the variance covariance matrix of VAR residuals)
#     with either the identity (giving I_tilde) or the lag-one autocovariance generating matrix M (giving M_tilde)
M_obj<-M_func(L,Sigma)
M_tilde<-M_obj$M_tilde
I_tilde<-M_obj$I_tilde
# Variance of target series: computed by M-SSA
diag(var_target)
# Variance of classic MSE predictors: formula based on M-SSA paper
diag(t(gammak_mse)%*%I_tilde%*%gammak_mse)
# Variance of M-SSA: in general variances of M-SSA <= variances MSE <= variances target (due to zero-shrinkage)
diag(t(bk_mat)%*%I_tilde%*%bk_mat)
# Compare true and sample estimates: the following two numbers match (sample converges to true for increasing sample length)
#   The m-th entry of the above vector should match the sample variance of the M-SSA output of the m-th series, see simulation above
diag(t(bk_mat)%*%I_tilde%*%bk_mat)[m]
var(na.exclude(y))
# Similarly, the m-th variance of the target should match the sample variance of zdelta, see above simulation experiment
diag(var_target)[m]
var(na.exclude(zdelta))
# Summary: we verified (some of) the theoretical expressions in the M-SSA paper:
#   -Sample estimates of variances converge to expected values
# We can now compute the true lag one ACFs of the classic MSE nowcast (look at the M-SSA paper for technical details)
rho_mse<-gammak_mse[,1]%*%M_tilde%*%gammak_mse[,1]/gammak_mse[,1]%*%I_tilde%*%gammak_mse[,1]
for (i in 2:n)
rho_mse<-c(rho_mse,gammak_mse[,i]%*%M_tilde%*%gammak_mse[,i]/gammak_mse[,i]%*%I_tilde%*%gammak_mse[,i])
# Similarly, we can compute the true lag-one ACFs of M-SSA, see M-SSA paper
rho_ssa<-bk_mat[,1]%*%M_tilde%*%bk_mat[,1]/bk_mat[,1]%*%I_tilde%*%bk_mat[,1]
for (i in 2:n)
rho_ssa<-c(rho_ssa,bk_mat[,i]%*%M_tilde%*%bk_mat[,i]/bk_mat[,i]%*%I_tilde%*%bk_mat[,i])
# We can also derive the HTs based on the above ACFs: HT and lag-one ACFs are linked bijectively (at least for Gaussian processes)
ht_comp<-apply(matrix(rho_ssa,nrow=1),1,compute_holding_time_from_rho_func)[[1]]$ht
ht_comp
# We have verified in the above simulation experiment that sample HTs match these `true' numbers
# We can once again verify successful numerical optimization
#  -If successful, then ht_comp (based on optimized filters) should match the imposed HTs ht_mssa_vec
#  -Increasing the size of split_grid (the number of iterations) tightens the fit between ht_comp above and ht_mssa_vec below
# The following (absolute) differences should vanish: increasing split_grid reduces the error
abs(ht_mssa_vec-ht_comp)
# We can also compute HTs of the classic M-MSE benchmark:
#   -In general M-SSA is designed to be smoother (stronger noise suppression), i.e., ht_mssa_vec is larger than the below HTs of MSE design
#   -We can of course change the HT in the constraint as specified in the call to M-SSA
apply(matrix(rho_mse,nrow=1),1,compute_holding_time_from_rho_func)[[1]]$ht
# Next we can compute the target correlations
#   -As explained above, we here compute the correlation of M-SSA with M-MSE benchmark (instead of acausal target)
#     -Targeting the two-sided filter is formally equivalent to targeting the classic M-MSE
#   -If HT of M-SSA matches HT of M-MSE, then M-SSA replicates the latter (in this case, the `target correlation' would be one)
#   -If the imposed HT of M-SSA is larger than HT of M-MSE, then the target correlation is smaller one
#     -M-SSA maximizes this correlation subject to the HT constraint
# -See formula in M-SSA paper. In R-code we obtain:
crit_mse<-gammak_mse[,1]%*%I_tilde%*%gammak_mse[,1]/gammak_mse[,1]%*%I_tilde%*%gammak_mse[,1]
for (i in 2:n)
crit_mse<-c(crit_mse,gammak_mse[,i]%*%I_tilde%*%gammak_mse[,i]/gammak_mse[,i]%*%I_tilde%*%gammak_mse[,i])
# The correlation of the MSE-benchmark with itself is trivially one
crit_mse
# M-SSA tries to maximize this correlation  subject to the HT constraint
crit_ssa<-gammak_mse[,1]%*%I_tilde%*%bk_mat[,1]/(sqrt(bk_mat[,1]%*%I_tilde%*%bk_mat[,1])*sqrt(gammak_mse[,1]%*%I_tilde%*%gammak_mse[,1]))
for (i in 2:n)
crit_ssa<-c(crit_ssa,gammak_mse[,i]%*%I_tilde%*%bk_mat[,i]/(sqrt(bk_mat[,i]%*%I_tilde%*%bk_mat[,i])*sqrt(gammak_mse[,i]%*%I_tilde%*%gammak_mse[,i])))
crit_ssa
criterion_mat<-rbind(crit_mse,crit_ssa)
colnames(criterion_mat)<-c(paste("Series ",1:n,paste=""))
rownames(criterion_mat)<-c("MSE","SSA")
# Correlations with classic MSE predictor
# M-SSA maximizes these numbers (equivalent objective function)
criterion_mat
# Compare the second row of this matrix with MSSA_obj$crit_rhoyz computed by M-SSA
MSSA_obj$crit_rhoyz
#  crit_rhoyz is the objective function of the optimization criterion and is maximized by M-SSA
# We verified convergence of sample estimates in exercise 1 and we cross-checked some of the expressions in the M-SSA paper
#   -We shall complete this experiment in exercise 3 below
#   -In particular, we shall see that sample estimates of the (target-) correlations will converge
#     towards the above numbers when targeting either the acausal filter or the causal M-MSE
##########################################################################################
# Summary:
# -M-SSA has a rich output with additional filters (including M-MSE) and additional performance metrics
# -Theoretical expressions (expected values: see M-SSA paper) match sample estimates (for sufficiently long samples)
# -M-SSA optimization concept:
#   -We verified that M-SSA maximizes the target correlation conditional on the HT constraint
#   -The target correlation can be defined with respect to the effective acausal target or M-MSE: the M-SSA solution will be the same
#   -The target correlation ignores static level and scale adjustments but is otherwise equivalent to minimum MSE
# -M-SSA replicates classic M-MSE signal extraction filters (up to static level adjustment)  by
#   inserting the HT of the latter into the M-SSA constraint
# -M-SSA can address backcasting (delta<0), nowcasting (delta=0) and forecasting (delta>0)
# -The target specification is generic: in the above experiment we relied on the two-sided HP
#   -Classic h-step ahead forecasting can be obtained by replacing the HP-filter by the identity (see univariate SSA tutorials on the topic)
#   -We could insert Hamilton or Baxter-King or Beveridge-Nelson specifications, see earlier tutorials
# -The data generating process (DGP) is assumed to be stationary (could be generalized); otherwise the specification is general
#   -In our applications we typically consider growth-dynamics, i.e., data in first differences (differenced data is close to stationarity)
#   -M-SSA relies on the Wold-decomposition of the (stationary) DGP which is straightforward to obtain for a VARMA process (MA-inversion)
# -A convergence of sample performances towards expected numbers assumes the model to be `true'
#   -We shall see that the application to German macro data (tutorials 7.2 and 7.3) is remarkably robust
#     -against singular Pandemic data (outliers)
#     -against in-sample span for VAR: pre-financial crisis M-SSA (data up Jan-2007) performs nearly as well as full sample M-SSA
#     -against VARMA specification (as long as heavy overfitting is avoided)
# -Tutorial 7.2 will demonstrate that the VAR(1) is (most likely) misspecified, as we might already suspect
#   -We then provide a simple and effective trick to overcome the misspecification in the context of this
#     macro-application
###########################################################################################
# Exercise 3: to conclude this tutorial we wrap the above code into functions and we verify some additional
#   convergence of sample estimates to theoretical performance numbers
# Wrappers: let's pack the above code into functions with distinct tasks
# 1. Target function
HP_target_sym_T<-function(n,lambda_HP,L)
{
HP_obj<-HP_target_mse_modified_gap(L,lambda_HP)
hp_symmetric=HP_obj$target
hp_classic_concurrent=HP_obj$hp_trend
hp_one_sided<-HP_obj$hp_mse
# Target first series
gamma_target<-c(hp_one_sided,rep(0,(n-1)*L))
# We now proceed to specifying the targets of the remaining n-1 series
for (i in 2:n)
gamma_target<-rbind(gamma_target,c(rep(0,(i-1)*L),hp_one_sided,rep(0,(n-i)*L)))
# The above target filters are one-sided (right half of two-sided filter)
# We now tell M-SSA that it has to mirror the above filters at their center points to obtain two-sided targets
symmetric_target<-T
return(list(gamma_target=gamma_target,symmetric_target=symmetric_target))
}
# 2. MA-inversion as based on VAR model
MA_inv_VAR_func<-function(Phi,Theta,L,n,Plot=F)
{
# MA inversion of VAR
# MA inversion is used because the M-SSA optimization criterion relies an white noise
#   For autocorrelated data, we thus require the MA-inversion of the DGP
xi_psi<-PSIwgt(Phi = Phi, Theta = NULL, lag = L, plot = F, output = F)
xi_p<-xi_psi$psi.weight
# Transform Xi_p into Xi as structured/organized for M-SSA
#   First L entries, from left to right, are weights of first explanatory series, next L entries are weights of second WN
xi<-matrix(nrow=n,ncol=n*L)
for (i in 1:n)
{
for (j in 1:L)
xi[,(i-1)*L+j]<-xi_p[,i+(j-1)*n]
}
if (Plot)
{
# Plot MA inversions
par(mfrow=c(1,n))
for (i in 1:n)#i<-1
{
mplot<-xi[i,1:min(10,L)]
for (j in 2:n)
{
mplot<-cbind(mplot,xi[i,(j-1)*L+1:min(10,L)])
}
ts.plot(mplot,col=rainbow(ncol(mplot)),main=paste("MA inversion ",colnames(x_mat)[i],sep=""))
}
}
return(list(xi=xi))
}
# M-SSA
MSSA_main_func<-function(delta,ht_vec,xi,symmetric_target,gamma_target,Sigma,Plot=F)
{
# Compute lag-one ACF corresponding to HT in M-SSA constraint: see previous tutorials on the link between HT and lag-one ACF
rho0<-compute_rho_from_ht(ht_vec)$rho
# Some default settings for numerical optimization
# with_negative_lambda==T allows the extend the search to unsmoothing (generate more zero-crossings than benchmark):
#   Default value is FALSE (smoothing only)
with_negative_lambda<-F
# Default setting for numerical optimization
lower_limit_nu<-"rhomax"
# Optimization with half-way triangulation: effective resolution is 2^split_grid. Much faster than brute-force grid-search.
# 20 is a good value: fast and strong convergence in most applications
split_grid<-20
# M-SSA wants the target with rows=target-series and columns=lags: for this purpose we here transpose the filter
gamma_target<-t(gamma_target)
# Now we can apply M-SSA
MSSA_obj<-MSSA_func(split_grid,L,delta,grid_size,gamma_target,rho0,with_negative_lambda,xi,lower_limit_nu,Sigma,symmetric_target)
# In principle we could retrieve filters, apply to data and check performances
# But M-SSA delivers a much richer output, containing different filters and useful evaluation metrics
# These will be analyzed further down
# So let's pick out the real-time filter
bk_x_mat<-MSSA_obj$bk_x_mat
if (Plot)
{
par(mfrow=c(1,n))
for (i in 1:n)# i<-1
{
mplot<-bk_x_mat[1:L,i]
for (j in 2:n)
{
mplot<-cbind(mplot,bk_x_mat[(j-1)*L+1:L,i])
}
ts.plot(mplot,main=paste("MSSA applied to x ",colnames(x_mat)[i],sep=""),col=rainbow(n))
}
}
# We return the M-SSA filter as well as the whole M-SSA object which hides additional useful objects
return(list(bk_x_mat=bk_x_mat,MSSA_obj=MSSA_obj))
}
# 4. Filter function: apply M-SSA filter to data and compute target
filter_func<-function(x_mat,bk_x_mat,gammak_x_mse,gamma_target,symmetric_target,delta)
{
len<-nrow(x_mat)
n<-dim(bk_x_mat)[2]
# Compute M-SSA filter output
mssa_mat<-mmse_mat<-target_mat<-NULL
for (m in 1:n)
{
bk<-NULL
# Extract coefficients applied to m-th series
for (j in 1:n)#j<-2
bk<-cbind(bk,bk_x_mat[((j-1)*L+1):(j*L),m])
y<-rep(NA,len)
for (j in L:len)#j<-L
{
y[j]<-sum(apply(bk*(x_mat[j:(j-L+1),]),2,sum))
}
mssa_mat<-cbind(mssa_mat,y)
}
# Compute M-MSE: classic MSE signal extraction design
for (m in 1:n)
{
gamma_mse<-NULL
# Extract coefficients applied to m-th series
for (j in 1:n)#j<-2
gamma_mse<-cbind(gamma_mse,gammak_x_mse[((j-1)*L+1):(j*L),m])
ymse<-rep(NA,len)
for (j in L:len)#j<-L
{
ymse[j]<-sum(apply(gamma_mse*(x_mat[j:(j-L+1),]),2,sum))
}
mmse_mat<-cbind(mmse_mat,ymse)
}
# Apply target to m-th-series
target_mat<-NULL
for (m in 1:n)#
{
# In general, m-th target is based on j=1,...,n filters applied to explanatory variables j=1,...,n
gammak<-NULL
for (j in 1:n)
{
# Retrieve j-th filter for m-th target
gammak<-cbind(gammak,gamma_target[(j-1)*L+1:L,m])
}
z<-rep(NA,len)
if (symmetric_target)
{
# Here the right half of the filter is mirrored to the left at its peak
# Moreover, the data is shifted by delta
for (j in (L-delta):(len-L-delta+1))#j<-L-delta
z[j]<-sum(apply(gammak*x_mat[delta+j:(j-L+1),],2,sum))+sum(apply(gammak[-1,]*x_mat[delta+(j+1):(j+L-1),],2,sum))
} else
{
# Data shifted by delta: we do not mirror filter weights
for (j in (L-delta):(len-delta))
{
z[j]<-sum(apply(gammak*(x_mat[delta+j:(j-L+1),]),2,sum))
}
}
names(z)<-names(y)<-rownames(x_mat)
target_mat<-cbind(target_mat,z)
}
colnames(mssa_mat)<-colnames(mmse_mat)<-colnames(target_mat)<-colnames(x_mat)
return(list(mssa_mat=mssa_mat,target_mat=target_mat,mmse_mat=mmse_mat))
}
#------------------------------------------------------------------------
# Let's now apply the above functions to the previous simulation experiment
# 1. Target
lambda_HP<-160
# Filter length: roughly 4 years. The length should be an odd number, see above comments (mirroring)
L<-31
target_obj<-HP_target_sym_T(n,lambda_HP,L)
gamma_target=t(target_obj$gamma_target)
symmetric_target=target_obj$symmetric_target
# Target as applied to original data (not MA-inversion)
# To obtain the two-sided filter, the right tail will be mirrored to the left, about the center point
par(mfrow=c(1,1))
ts.plot(gamma_target,col=rainbow(n),main="Target as applied to original data: right tail will be mirrored to the left to obtain two-sided HP")
abline(v=1+(1:n*(nrow((gamma_target))/n)))
# If the boolean symmetric_target is true, the right tail of the (one-sided) target will be mirrored to
#   the left of the center point to obtain the two-sided design
symmetric_target
# 2. MA-inversion as based on VAR model
MA_inv_obj<-MA_inv_VAR_func(Phi,Theta,L,n,T)
xi<-MA_inv_obj$xi
# 3. M-SSA function
# Nowcast
delta<-0
# One year ahead forecast for quarterly data
delta<-4
MSSA_main_obj<-MSSA_main_func(delta,ht_mssa_vec,xi,symmetric_target,gamma_target,Sigma,T)
MSSA_main_obj$bk_x_mat=bk_x_mat
MSSA_obj=MSSA_main_obj$MSSA_obj
# Benchmark MSE predictor
gammak_x_mse<-MSSA_obj$gammak_x_mse
# 4. Filter function: apply M-SSA filter to data
# For long samples the execution may require some patience (because of the for-loops which take time to process in R)
filt_obj<-filter_func(x_mat,bk_x_mat,gammak_x_mse,gamma_target,symmetric_target,delta)
# retrieve M-SSA and acausal target
mssa_mat=filt_obj$mssa_mat
target_mat=filt_obj$target_mat
mmse_mat<-filt_obj$mmse_mat
#------------------------
# Checks: the obtained output should be identical to previous y and zdelta for series m_check: differences should vanish
max(abs(y-mssa_mat[,m_check]),na.rm=T)
max(abs(zdelta-target_mat[,m_check]),na.rm=T)
# Mean-square errors
apply(na.exclude((target_mat-mssa_mat)^2),2,mean)
# We can verify that the sample target correlations between effective acausal target and M-SSA converge towards
#   expectations for increasing sample length
for (i in 1:n)
print(cor(na.exclude(cbind(target_mat[,i],mssa_mat[,i])))[1,2])
# Sample estimates should be close to true values:
MSSA_obj$crit_rhoy_target
# Similarly, we can verify that the sample correlations between M-MSE and M-SSA converge to criterion values
#   for increasing sample size len
for (i in 1:n)
print(cor(na.exclude(cbind(mmse_mat[,i],mssa_mat[,i])))[1,2])
# Sample estimates should be close to true values (objective function of M-SSA):
MSSA_obj$crit_rhoyz
# M-SSA optimizes the target correlation (between M-SSA and M-MSE) under the holding time constraint:
# We can compare sample and expected (imposed) HTs:
apply(mssa_mat,2,compute_empirical_ht_func)
ht_mssa_vec
# The above functions can also be sourced
source(paste(getwd(),"/R/M_SSA_utility_functions.r",sep=""))
# We shall rely on these functions in tutorials 7.2 and 7.3
rm(list=ls())
# Load the required R-libraries
# Standard filter package
library(mFilter)
# Multivariate time series: VARMA model for macro indicators: used here for simulation purposes only
library(MTS)
# HAC estimate of standard deviations in the presence of autocorrelation and heteroscedasticity
library(sandwich)
# Extended time series
library(xts)
# Library for Diebold-Mariano test of equal forecast performance
library(multDM)
# GARCH model: for improving regression estimates
library(fGarch)
# Load the relevant M-SSA functionalities
# M-SSA functions
source(paste(getwd(),"/R/functions_MSSA.r",sep=""))
# Load signal extraction functions used for JBCY paper (relies on mFilter)
source(paste(getwd(),"/R/HP_JBCY_functions.r",sep=""))
# Utility functions for M-SSA, see tutorial
source(paste(getwd(),"/R/M_SSA_utility_functions.r",sep=""))
# Set of performance metrics and tests of unequal predictability
#source(paste(getwd(),"/R/performance_statistics_functions.r",sep=""))
#------------------------------------------------------------------------
# Load data and select indicators: see tutorial 7.2 for background
load(file=paste(getwd(),"\\Data\\macro",sep=""))
tail(data)
lag_vec<-c(2,rep(0,ncol(data)-1))
# Note: we assume a publication lag of two quarters for BIP, see the discussion in tutorial 7.2
tail(data)
# Tutorial 7.2: application of the M-SSA to quarterly macro-data
# The previous tutorial 7.1 demonstrates asymptotic convergence of the relevant performance numbers to
#   expected values (as derived in the M-SSA paper), assuming knowledge of the data generating process (DGP)
# -Tutorial 7.1 checks the theory
# In contrast, we here have relatively short (in-sample) spans and we (have to) allow for (and address)
#   model misspecification
# Purposes of this tutorial:
# -Apply the M-SSA to quarterly (German) macro-data during insecure times
#   -Tutorials 7.1-7.3 were written early 2025 and rely on data up to Jan-2025
#   -The HP-filter signals a severe on-going (and worsening) recession, see exercise 4 below
#     -Germany endures a recession: we observe negative BIP-growth since several quarters
#   -Can we use M-SSA to predict future BIP dynamics: what are the prospects for 2025 and 2026?
# -Analyze various designs for nowcasting and forecasting German GDP (BIP:=Brutto Inland Produkt)
# -Analyze effects of model misspecifications (the VAR(1) cannot render recessions properly)
# -Infer possible solutions for eluding model misspecification issues and analyze their efficacity
# -Provide an empirical background and basic insights to understand the various forecast designs
#   proposed in tutorial 7.3
# Some background information:
# -As discussed in tutorial 7.1, we do not deliver `GDP numbers' ("give me the number")
#   -Such a `number' would be subject to a forecast interval whose width would invariably invalidate its
#     relevance.
#   -Forecasting GDP `numbers' is almost surely (with probability one) a futile exercise,
#     at least in a multi-step  ahead perspective (several quarters ahead)
# -We here focus on looking ahead (sensing) the future growth dynamics as contained (but masked/hidden)
#     in present-day data: we try to `extract the minute signal` and `skip the dominating noise'
#   -The reliance in a `signal' may be anchored in the concept of `business cycle'
#   -In his talk at the University of Chicagoâ€™s Booth School of Business (March 07, 2025), FED-chair Jerome Powell
#       said: "As we parse the incoming information, we are focused on separating the signal from the noise
#       as the outlook evolves", suggesting in his talk that the FED should not (over)react to noise.
#   -Similarly, a forecast procedure should not be reactive to `noise'
#   -However, when targeting BIP in its entirety, including its noisy part, classic direct forecasts are
#     subject to overfitting, mainly because the components that dominate the MSE (the erratic
#     high-frequency pulses) distract the OLS optimization from fitting the (much weaker) systematic dynamics
# -M-SSA in this application is about dynamic aspects of prediction:
#     -M-SSA emphasizes the target correlation, thereby ignoring `static' level and scale adjustments (calibration), needed to generate GDP `numbers'
#     -We derive predictors which do not emphasize unilaterally a mean-square error metric (we already know the outcome of doing so)
#       -Instead, M-SSA emphasizes ALSO left-shift/lead/advancement and smoothness (few false alarms): AST trilemma
# -We try to address questions like:
#   -Did we reach the bottom of the current recession in Germany (based on data up to Jan-2025)?
#     -The HP filter suggests that we did not, see exercise 4 below
#   -Is the economy currently recovering (Jan 2025)?
#   -Can we expect to reach above long-term growth in foreseeable time?
# -To be clear: don't ask for a `number'
#   -In our plots, predictors are standardized and positive/negative readings suggest above/below
#     long-term growth
#   -M-SSA controls the rate of zero-crossings, i.e., the rate of crossings of the predictors above or
#     below long-term average growth
#   -Therefore we can control for the number of noisy (false) alarms (signaling below or above long-term growth)
# -BIP `numbers' could be obtained by calibration of the standardized M-SSA predictors on BIP
#   -Determine optimal static level and scale adjustments by linear regression
# The tutorial is structured into four exercises
# -Exercise 1 discusses important design decisions (target specification) and applies M-SSA to the data
#   -Unfortunately, the VAR(1) model is subject to misspecification resulting in poor performances
# -Exercise 2 will analyze the (main) causes of misspecification and propose solutions to overcome undesirable consequences
# -Exercise 3 combines all ingredients to propose a recipe for constructing the M-SSA BIP predictors
# -Exercise 4 compares our results with the HP-filter
#   -M-SSA contradicts HP (and the future will tell)
#----------------------
# Start with a clean sheet
rm(list=ls())
# Let's start by loading the required R-libraries
# Standard filter package
library(mFilter)
# Multivariate time series: VARMA model for macro indicators: used here for simulation purposes only
library(MTS)
# HAC estimate of standard deviations in the presence of autocorrelation and heteroscedasticity
library(sandwich)
# Load the relevant M-SSA functionalities
# M-SSA functions
source(paste(getwd(),"/R/functions_MSSA.r",sep=""))
# Load signal extraction functions used for JBCY paper (relies on mFilter)
source(paste(getwd(),"/R/HP_JBCY_functions.r",sep=""))
# Useful M-SSA wrappers
source(paste(getwd(),"/R/M_SSA_utility_functions.r",sep=""))
#------------------------------------------------------------------------
# Exercise 1: apply M-SSA to quarterly German Macro-data
# 1.1. Load data and select indicators
# 1.1.1 We first look at the original files: BIP `numbers` refer to this data
data_file_name<-c("Data_HWI_2025_02.csv","gdp_2025_02.csv")
# Monthly data: note that ip is generally not available up to sample end (publication lag)
data_monthly<-read.csv(paste(getwd(),"/Data/",data_file_name[1],sep=""))
tail(data_monthly)
# Quarterly data: BIP in the first data column
data_quarterly<-read.csv(paste(getwd(),"/Data/",data_file_name[2],sep=""))
tail(data_quarterly)
