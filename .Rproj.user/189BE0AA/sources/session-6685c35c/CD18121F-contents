> tail(perf_obj$epsilon_oos)
2023-01-01  2023-04-01  2023-07-01  2023-10-01  2024-01-01  2024-04-01 
0.01790460 -0.37197076  0.23715564 -0.27987976  0.04445862 -0.41459247 
> # We also obtain the out-of-sample forecast errors of the mean benchmark predictor
  > tail(perf_obj$epsilon_mean_oos)
2023-01-01  2023-04-01  2023-07-01  2023-10-01  2024-01-01  2024-04-01 
-0.10228462 -0.51156808 -0.06393151 -0.45140257 -0.15349039 -0.37610470 
> # More importantly, the function computes HAC-adjusted p-values of the regression of the out-of-sample predictor (oos_pred_wls obtained in exercise 3.3.3 above)
  > #   on forward-shifted BIP
  > perf_obj$p_value
[1] 0.02040858
> # The same but without singular Pandemic readings
  > perf_obj$p_value_without_covid
[1] 0.02635646
> 
  > # In addition, we obtain the out-of-sample MSE of the M-SSA component predictor (the mean of oos_error_wls^2, where oos_error_wls was obtained in exercise 3.3.3 above)
  > perf_obj$MSE_oos
[1] 0.629429
> # The same but without Pandemic: we can s(e)ize the impact of the crisis on the MSE metric!
  > perf_obj$MSE_oos_without_covid
[1] 0.3767111
> # The function also computes the out-of-sample MSE of the simple mean benchmark predictor (expanding window): 
  > #   -we expect the mean-benchmark to be slightly worse (larger MSE) than the M-SSA components, at least for 
  > #     smaller forward-shifts of BIP (in the long run, the mean is difficult to outperform)
  > perf_obj$MSE_mean_oos
[1] 0.6783069
> perf_obj$MSE_mean_oos_without_covid
[1] 0.4263043
> 
  > # We can also compute the rRMSE of the new M-SSA component predictor against the simple mean benchmark
  > sqrt(perf_obj$MSE_oos/perf_obj$MSE_mean_oos)
[1] 0.9632971
> # Same but without Pandemic
  > sqrt(perf_obj$MSE_oos_without_covid/perf_obj$MSE_mean_oos_without_covid)
[1] 0.9400357